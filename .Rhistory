# tweets %>% filter(id_str == "936551346299338752")  %>% pull(text)
# tweets %>% filter(id_str == "822501939267141634")  %>% pull(text)
# tweets %>% filter(id_str == "980762392303980544")  %>% pull(text)
# tweets %>% filter(id_str == "931477193255014400")  %>% pull(text)
# tweets %>% filter(id_str == "1011952266268545024")  %>% pull(text)
topic_per_tweet
split_topics
split_topics$1
split_topics[[1]]
topics(mod)
topics(lda_model)
topics(lda_model) %>% nrow()
topics(lda_model) %>% length()
tweets$text %>% length()
sample_1
tweets %>% filter(id_str %in% sample_1$topic)
sample_1$topic
sample_1 %>% rownames()
tweets %>% filter(id_str %in% sample_1 %>% rownames())
sample_1 %>% rownames()
tweets %>% filter(id_str %in% (sample_1 %>% rownames()))
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'1'
set.seed(100)
sample_1 <- sample_n(topic_1, 5)
(sample_id <- row.names(sample_1))
# Checking those specific documents - 3/5 of the tweets relate directly to the expected topic of immigration
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'1'
set.seed(100)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
# Checking those specific documents - 3/5 of the tweets relate directly to the expected topic of immigration
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'2'
set.seed(100)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
# Checking those specific documents - 3/5 of the tweets relate directly to the expected topic of immigration
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
posterior(lda_model)
a <- posterior(lda_model)
a
a$terms
a$topics
a$terms %>% dim()
a$opics %>% dim()
a$topics %>% dim()
theta
a$topics %>% select(id_str, topic)
a$topics
tweets %>% select(id_str, date)
res <- posterior(lda_model)
res$topics
rownames(res$topics)
tibble(id_str = rownames(res$topics), tw = res$topics[,2])
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = mean)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
scale_x_date(date_labels="%b %y",date_breaks  ="1 month") +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-1 (Immigration) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = Topic)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
scale_x_date(date_labels="%b %y",date_breaks  ="1 month") +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-1 (Immigration) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = Topic)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
#scale_x_date(date_labels="%b %y",date_breaks  ="1 month") +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-1 (Immigration) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = Topic)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-1 (Immigration) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = Topic)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-2 (Economy) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
terms(mod,15)[,2]
terms(lda_model,15)[,2]
res$terms[2,] %>% sort(decreasing = T) %>% `[`(1:15)
tradew <- res$terms[,colnames(res$terms) == "trade"]
tradew <- tradew/sum(tradew)
tradew
tradew <- res$terms[,colnames(res$terms) == "trade"]
tradew <- tradew/sum(tradew)
htopic <- which.max(tradew)
tradew
htopic
res$terms[htopic,] %>% sort(decreasing = T) %>% `[`(1:15)
?LDA
dtm.new
library("quanteda", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("topicmodels", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("stm", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("tidyverse", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("lubridate", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("dplyr", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("data.table", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
# Reading tweets
tweets <- readr::read_csv("../../../pset_data-master/trump-tweets.csv", col_types="cTDc")
# Plotting histogram
tweets %>% mutate(date = floor_date(date, "month")) %>%
group_by(date) %>%
summarise(twt = length(text)) %>%
ggplot(aes(x=date, y=twt)) + geom_col()
# Creating corpus
tweets_corpus <- tweets %>%
corpus() %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(c(stopwords("en"), "Trump", "President", "rt", "amp", "great", "make", "america", "again", "\U0001f1fa\U0001f1f8")) %>%
tokens_ngrams(n = 1:2) # up to bigrams
# Setting docnames to the tweet id
docnames(tweets_corpus) <- tweets$id_str
# Creating DFM
tweets_dfm <- tweets_corpus %>% dfm() %>% dfm_trim(min_termfreq = 3)
tweets_dfm <- tweets_dfm[,sort(featnames(tweets_dfm))]
# Converting to topic model
tmdfm <- quanteda::convert(tweets_dfm, to = "tm")
raw.sum <- apply(tmdfm,1,FUN=sum) # sum by raw each raw of the table
dtm.new   <- tmdfm[raw.sum> 0, ] # removing those with zero
# Estimating the LDA model
lda_model <- LDA(dtm.new, k = 10, control = list(alpha=0.1, seed = 88))
# Extracting top terms for each topic
terms(lda_model, 10) # top 10 terms - here we can for example labels topic 1 as immigration, topic 2 as media/news, topic 4 as rebublican policy, and topic 7 as the economy and topic 8 as foreign relations.
# Creating corpus
tweets_corpus <- tweets %>%
corpus() %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(c(stopwords("en"), "Trump", "President", "rt", "amp", "great", "make", "america", "again", "\U0001f1fa\U0001f1f8")) %>%
tokens_ngrams(n = 1:2) # up to bigrams
# Setting docnames to the tweet id
docnames(tweets_corpus) <- tweets$id_str
# Creating DFM
tweets_dfm <- tweets_corpus %>% dfm() %>% dfm_trim(min_termfreq = 3)
tweets_dfm <- tweets_dfm[,sort(featnames(tweets_dfm))]
# Converting to topic model
tmdfm <- quanteda::convert(tweets_dfm, to = "tm")
raw.sum <- apply(tmdfm,1,FUN=sum) # sum by raw each raw of the table
dtm.new   <- tmdfm[raw.sum> 0, ] # removing those with zero
# Estimating the LDA model
lda_model <- LDA(dtm.new, k = 10, control = list(alpha=0.1, seed = 95))
# Extracting top terms for each topic
terms(lda_model, 10) # top 10 terms - here we can for example labels topic 1 as immigration, topic 2 as media/news, topic 4 as rebublican policy, and topic 7 as the economy and topic 8 as foreign relations.
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'8'
set.seed(100)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'8'
set.seed(101)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'8'
set.seed(105)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,8])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = Topic)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-2 (Economy) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
terms(lda_model,15)[,8]
res$terms[2,] %>% sort(decreasing = T) %>% `[`(1:15)
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'2'
set.seed(105)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'2'
set.seed(101)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
# Looking at the topic with highest proportion for each tweet
topic_per_tweet <- data.frame(topic = topicmodels::topics(lda_model))
# Splitting per topic
split_topics <- split(topic_per_tweet, topic_per_tweet$topic)
# Creating df with topic q and sampling from that
topic_1 <- split_topics$'2'
set.seed(95)
sample_1 <- sample_n(topic_1, 5)
sample_id <- row.names(sample_1)
tweets %>% filter(id_str %in% (sample_1 %>% rownames())) %>% select(text) %>% pull()
res <- posterior(lda_model)
t_weights <- tibble(id_str = rownames(res$topics), tw = res$topics[,2])
dprop <- left_join(t_weights, tweets %>% select(id_str, date), by = "id_str")
out <- dprop %>% mutate(date = floor_date(date), "month") %>%
group_by(date) %>%
summarise(Topic=mean(tw))
ggplot(out, aes(x = date, y = Topic)) + geom_line() +
scale_y_continuous(name = "Average Share", labels = scales::percent) +
theme(panel.border = element_rect(color="black", size=0.5, linetype="solid", fill = NA),
panel.grid.major = element_line(colour = "grey96"),panel.grid.minor = element_line(colour = "grey96"), axis.text.x = element_text(size = 6), axis.title.x = element_blank()) +
labs(title = "Avg. topic-share of Trump tweets on Topic-2 (Immigration) Jan.2017 - Jun.2018",
subtitle = "Grouped by monthly average")
terms(lda_model,15)[,2]
res$terms[2,] %>% sort(decreasing = T) %>% `[`(1:15)
tradew <- res$terms[,colnames(res$terms) == "trade"]
tradew <- tradew/sum(tradew)
htopic <- which.max(tradew)
tradew
res$terms[htopic,] %>% sort(decreasing = T) %>% `[`(1:15)
mat <- matrix(c(0, 0.5, 0.5, 0, 0,
0, 1, 0, 0, 0,
0, 0, 0, 0.5, 0.5,
0, 0, 0, 1, 0,
0, 0, 0, 0, 1), nrow = 5, byrow=T)
diag(5)-mat
solve(diag(5)-mat)
mat <- matrix(c(0, 0.5, 0.5, 0, 0,
0, 0, 0, 0, 0,
0, 0, 0, 0.5, 0.5,
0, 0, 0, 0, 0,
0, 0, 0, 0, 0), nrow = 5, byrow = T)
solve(diag(5)-mat)
1/2 + 1/4 + 1/4
1/2 + 1/4 + 1/8
2
2.5
0.75
3.25
99/2
46.25
x <- 5
y <- 4
k <- 3
choose(x+y-k-1, y-1)
choose(x+y-1, x-1)
choose(x+y-k-1, x-1)
choose(x+y-k-1, x-1)
choose(x+y-1, x-k)
choose(x+y-k-1, x-1)
choose(x+y-1, x-k)
choose(x+y-k-1, y-1)
choose(x+y-1, x-k)
50.000/12
18*0.08
38/18-1
0.53 * 298 +
0.59 * 454 +
0.55 * 523 +
0.65 * 589 +
0.69 * 640
1537.9/5
640*0.69 - 640*0.13
640-268.8
298-166.9
640-268
454-45.2
454-24.2
454-244.2
library(bannerCommenter)
library(tidyverse)
library(lubridate)
library(quanteda)
setwd("C:/Users/dordo/Dropbox/Capstone Project")
## Read returns data
data_ret <- read_csv("Data/S&P/LogReturnData.csv") %>%
select(1:2) %>% rename(Ret=`^OEX`) %>%
mutate(Dir=ifelse(sign(Ret) == 0, -1, sign(Ret))) %>%
filter(!is.na(Ret)) %>%
#filter(year(Date)<2019 | year(Date)>2020) %>%
arrange(Date)
library(bannerCommenter)
library(tidyverse)
library(lubridate)
library(quanteda)
setwd("C:/Users/dordo/Dropbox/Capstone Project")
## Read returns data
data_ret <- read_csv("Data/S&P/LogReturnData.csv") %>%
select(1:2) %>% rename(Ret=`^OEX`) %>%
mutate(Dir=ifelse(sign(Ret) == 0, -1, sign(Ret))) %>%
filter(!is.na(Ret)) %>%
#filter(year(Date)<2019 | year(Date)>2020) %>%
arrange(Date)
## Read Corpus data
data <- read_csv("Data/Twitter/TwitterConsolidated.csv") %>%
rename(Date=date) %>%
arrange(Date) %>%
mutate(twid = seq_along(tweet))
## IMPORTANT: Be careful with test set leakage
set.seed(10)
data_aux <- data %>%
mutate(Date = round_date(Date, unit = "day")) %>%
group_by(Date) %>%
group_modify(~slice_sample(.x, n=floor(nrow(.x)*0.1)))
## Join:
data_aux <- data_aux %>% select(twid, tweet, Date) %>% mutate(Date=as_date(Date))
## Auxiliary function to generate df
create_df <- function(x){
inner_join(data_aux %>% mutate(Date=Date + days(x)),
data_ret %>% select(Date, Dir))
}
## Data for contemporary + lag
all_data <- map(0:1, create_df)
get_seeds <- function(x, th){
## Generate corpus
corp <- corpus(x, text_field = "tweet")
## Create Tokens
toks <- corp %>% tokens(remove_punct = T, remove_numbers = T, remove_url = T) %>%
tokens_remove(setdiff(stopwords("en"), c("up","down")), padding = T) %>%
tokens_ngrams(1)
##----------------------------------------------------##
## Separate hashtags and mentions (TWITTER SPECIFIC)
toks_hts <- toks %>% tokens_select(pattern="#.+", valuetype="regex")
toks_mts <- toks %>% tokens_select(pattern = "@.+", valuetype = "regex")
## Remove numbers  hashtags and mentions
toks_oth <- toks %>% tokens_remove(pattern="#.+", valuetype="regex") %>%
tokens_remove(pattern="@.+", valuetype = "regex") %>%
tokens_remove("[0-9]+", valuetype ="regex")
rm(toks)
# Auxiliary functions to create each dataset
## Create dfm for each dataset
dfm_generator <- function(x, stp, ...){
dfm_hts <- dfm(x, remove = stp, ...)
dfm_trim(dfm_hts, min_termfreq = 15) ## Change threshold if needed: It means token has
## to appear 15 times in the corpus
}
## Get relative frequencies Up vs Down for each word in the dfm
dfm_proc <- function(x){
x %>% convert("data.frame") %>%
pivot_longer(-doc_id) %>%
group_by(name) %>%
arrange(name, doc_id)  %>%
mutate(freq = sum(value)) %>%
summarise(K = list(binom::binom.wilson(value, freq)[c("mean", "lower", "upper")])) %>%
unnest(c(K)) %>%
mutate("doc_id"=ifelse(seq_along(name) %% 2, "-1", "1"))
}
## Hashtags
hts <- c("#index", "#usmarkets", "#stockmarket", "s&p100", "#stockmarkets")
dfm_hts <- dfm_generator(toks_hts, hts, group = "Dir") %>% dfm_proc()
## Mentions
dfm_mts <- dfm_generator(toks_mts, NULL, group = "Dir") %>% dfm_proc()
## Create dfm for non hashtags (Removing stopwords)
dfm_corp <- dfm_generator(toks_oth,  NULL, group = "Dir") %>% dfm_proc()
##-------------------------------------------------##
## Filter seed words
get_seed <- function(x, th){
f <- function(x, y){((x$mean[1] < x$lower[2]) & (x$mean[2] > x$upper[1])) |
((x$mean[1] > x$upper[2]) & (x$mean[2] < x$lower[1]))}
use <- x %>% group_by(name) %>% group_map(f) %>% unlist()
x %>% bind_cols("Use"=rep(use,each=2)) %>%
filter(Use) %>%
filter(doc_id == "1") %>%
filter(mean < th | mean > (1-th)) %>%
mutate("Pol"=ifelse(mean > 0.5, "Positive", "Negative"))
}
## WE COULD CONSIDER Ngrams
seed_list <- list(dfm_corp, dfm_hts, dfm_mts) %>% map(get_seed, th = th) %>%
reduce(bind_rows)
seed_list %>% arrange(mean)
}
##-------------------------##
## Use the function on all data sources
res <- map(all_data, get_seeds, th = 0.3)
res
res[[1]]
res[[1]] %>% view()
res[[1]] %>% view()
library(tidyverse)
library(word2vec)
setwd("C:/Users/dordo/Documents/Daniel/LSE/MY 459/Proyecto")
model <- word2vec::read.word2vec("ModelWV.bin")
## record vocabulary
vocab <- summary(model)
## A la McDonald
root_seed <- c("^discov", "glut", "^overpro", "^oversup", "recess",
"^repair", "^surpl")
## Matches in the vocabulary
root_words <- unlist(lapply(root_seed, str_subset, string = vocab))
root_words
## Embedding for root words
good_embedding <- predict(model, root_words, "embedding")
good_embedding
## Check: Exploratory Factor Analysis of Data Matrices With More Variables Than Observations
good <- colMeans(good_embedding)
hist(good_embedding[,3])
good
root_seed_bad <- c("attack", "bomb", "^closur", "^concer", "confli",
"delay", "^dispu", "^disrup", "^explos", "fire",
"^hurri", "instab", "outa", "probl", "recover",
"sabo", "shorta", "storm", "strike", "turm",
"unres", "withd")
root_words_bad <- unlist(lapply(root_seed_bad, str_subset, string = vocab))
## Embedding for root words
bad_embedding <- predict(model, root_words_bad, "embedding")
bad <- colMeans(bad_embedding)
hist(bad_embedding[,3])
bad
##---------------------------------------------------------------------------------##
## Calculate semantic axis
sem_axis <- good-bad
sem_axis
dot_p <- t(sem_axis %*% t(as.matrix(model)))/50
dot_p
new_dict <- tibble("Word"=rownames(dot_p), "Sim"=dot_p[,1]) %>% arrange(desc(Sim))
new_dict
new_dict %>% arrange(sim)
new_dict %>% arrange(Sim)
root_words_bad
new_dict
root_words
sum(1:6)
sum(1:6)/6
exp(1)
70/2.71
77/2.71
W_2cTiV?.k&UDdd
97 + 100 /2
(97 + 100) /2
98.5*0.2
98.5*0.2 + 50 * 0.8
98.5*0.2 + 60 * 0.8
98.5*0.2 + 62 * 0.8
98.5*0.2 + 63 * 0.8
